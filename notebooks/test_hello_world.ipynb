{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2, 3])\n",
      "torch.Size([6]) torch.Size([6])\n",
      "tensor([1, 1, 1, 1, 2, 3])\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "from einops.einops import rearrange\n",
    "import torch\n",
    "mask = torch.randn((2,3,4))\n",
    "mask_v, all_j_ids = mask.max(dim=2)  # mask_v: [N,L]\n",
    "print(mask_v.shape,all_j_ids.shape)\n",
    "b_ids, i_ids = torch.where(mask_v) #i_ids : [N*L]\n",
    "print(b_ids.shape,i_ids.shape)\n",
    "j_ids = all_j_ids[b_ids, i_ids]\n",
    "print(j_ids)\n",
    "\n",
    "i_ids = torch.arange(0,5).long()\n",
    "print(i_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n",
      "torch.int64\n",
      "tensor([0, 0, 0, 1, 1, 1]) tensor([ 3,  2,  4,  5, 10, 11]) tensor([11,  1,  6,  7,  6,  1])\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "from einops.einops import rearrange\n",
    "import torch\n",
    "from kornia.utils import create_meshgrid\n",
    "N =2\n",
    "point2d = torch.Tensor([[[11.,3.3],[1,2.2],[5.6,3.8]],[[6.6,5.3],[5.5,9.8],[1.4,11.1]]])\n",
    "print(point2d.shape)\n",
    "\n",
    "point2d = point2d.round().long()\n",
    "b_x = torch.flatten(point2d[:,:,0])\n",
    "b_y = torch.flatten(point2d[:,:,1])\n",
    "\n",
    "b,xx = torch.where(point2d[:,:,0]>=0)\n",
    "print(xx.dtype)\n",
    "#\n",
    "device = point2d.device\n",
    "grid = torch.zeros(2,12,12,device=device)\n",
    "\n",
    "# print(mask)\n",
    "#\n",
    "#\n",
    "print(b,b_y,b_x)\n",
    "grid[b,b_y,b_x] =1\n",
    "print(grid)\n",
    "# torch.where(point2d)\n",
    "# print(point2d)\n",
    "\n",
    "\n",
    "# i_ids = torch.index_select(point2d, dim = 1, index = torch.tensor([0]))\n",
    "#\n",
    "# print(i_ids)\n",
    "# print('point2d')\n",
    "# print(point2d)\n",
    "# print(point2d.shape)\n",
    "\n",
    "# print(grid.shape)\n",
    "# grid[point2d]  =1\n",
    "# print(grid)\n",
    "\n",
    "\n",
    "# x2 = torch.Tensor([[ 0.,  1.,  2.],\n",
    "#                    [ 5.,  6.,  7.],\n",
    "#                    [10., 11., 12.],\n",
    "#                    [10., 11., 12.]])\n",
    "#\n",
    "# mask1 = torch.LongTensor([1,2])\n",
    "# mask2 = torch.LongTensor([2,2])\n",
    "# print(x2.shape)\n",
    "# print(mask1.shape)\n",
    "# x2[mask1,mask2] = -1\n",
    "# print(x2)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [2., 2., 2.]]]])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[[0.],\n",
      "          [0.]]],\n",
      "\n",
      "\n",
      "        [[[1.],\n",
      "          [2.]]]]),\n",
      "indices=tensor([[[[0],\n",
      "          [0]]],\n",
      "\n",
      "\n",
      "        [[[0],\n",
      "          [0]]]]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "x = torch.ones((2,1,2,3))\n",
    "heatmap_nms_batch = torch.cumsum(x,2)\n",
    "heatmap_nms_batch[::3] =0\n",
    "print(heatmap_nms_batch)\n",
    "pts_idx = heatmap_nms_batch[...].nonzero()  # [N, 4(batch, 0, y, x)]\n",
    "# print(pts_idx)\n",
    "y = torch.topk(heatmap_nms_batch,1,largest=True,sorted=True)\n",
    "print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y size torch.Size([1, 16, 2, 2])\n",
      "torch.Size([2, 16, 4])\n",
      "torch.Size([2, 4, 4])\n",
      "y size torch.Size([2, 16, 4])\n",
      "tensor([[[ 0.8415,  0.8415,  0.9093,  0.9093],\n",
      "         [ 0.5403,  0.5403, -0.4161, -0.4161],\n",
      "         [ 0.8415,  0.8415,  0.9093,  0.9093],\n",
      "         [ 0.5403,  0.5403, -0.4161, -0.4161],\n",
      "         [ 0.0998,  0.0998,  0.1987,  0.1987],\n",
      "         [ 0.9950,  0.9950,  0.9801,  0.9801],\n",
      "         [ 0.0998,  0.0998,  0.1987,  0.1987],\n",
      "         [ 0.9950,  0.9950,  0.9801,  0.9801],\n",
      "         [ 0.0100,  0.0100,  0.0200,  0.0200],\n",
      "         [ 0.9999,  0.9999,  0.9998,  0.9998],\n",
      "         [ 0.0100,  0.0100,  0.0200,  0.0200],\n",
      "         [ 0.9999,  0.9999,  0.9998,  0.9998],\n",
      "         [ 0.0010,  0.0010,  0.0020,  0.0020],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.0010,  0.0010,  0.0020,  0.0020],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000]],\n",
      "\n",
      "        [[ 0.9093,  0.9093,  0.9093,  0.9093],\n",
      "         [-0.4161, -0.4161, -0.4161, -0.4161],\n",
      "         [ 0.9093,  0.9093,  0.9093,  0.9093],\n",
      "         [-0.4161, -0.4161, -0.4161, -0.4161],\n",
      "         [ 0.1987,  0.1987,  0.1987,  0.1987],\n",
      "         [ 0.9801,  0.9801,  0.9801,  0.9801],\n",
      "         [ 0.1987,  0.1987,  0.1987,  0.1987],\n",
      "         [ 0.9801,  0.9801,  0.9801,  0.9801],\n",
      "         [ 0.0200,  0.0200,  0.0200,  0.0200],\n",
      "         [ 0.9998,  0.9998,  0.9998,  0.9998],\n",
      "         [ 0.0200,  0.0200,  0.0200,  0.0200],\n",
      "         [ 0.9998,  0.9998,  0.9998,  0.9998],\n",
      "         [ 0.0020,  0.0020,  0.0020,  0.0020],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000],\n",
      "         [ 0.0020,  0.0020,  0.0020,  0.0020],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PositionEncodingSine(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a sinusoidal position encoding that generalized to 2-dimensional images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_shape=(512, 512), temp_bug_fix=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_shape (tuple): for 1/8 featmap, the max length of 256 corresponds to 2048 pixels\n",
    "            temp_bug_fix (bool): As noted in this [issue](https://github.com/zju3dv/LoFTR/issues/41),\n",
    "                the original implementation of LoFTR includes a bug in the pos-enc impl, which has little impact\n",
    "                on the final performance. For now, we keep both impls for backward compatability.\n",
    "                We will remove the buggy impl after re-training all variants of our released models.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros((d_model, *max_shape))\n",
    "        y_position = torch.ones(max_shape).cumsum(0).float().unsqueeze(0)\n",
    "        x_position = torch.ones(max_shape).cumsum(1).float().unsqueeze(0)\n",
    "        if temp_bug_fix:\n",
    "            div_term = torch.exp(torch.arange(0, d_model//2, 2).float() * (-math.log(10000.0) / (d_model//2)))\n",
    "        else:  # a buggy implementation (for backward compatability only)\n",
    "            div_term = torch.exp(torch.arange(0, d_model//2, 2).float() * (-math.log(10000.0) / d_model//2))\n",
    "        div_term = div_term[:, None, None]  # [C//4, 1, 1]\n",
    "        pe[0::4, :, :] = torch.sin(x_position * div_term)\n",
    "        pe[1::4, :, :] = torch.cos(x_position * div_term)\n",
    "        pe[2::4, :, :] = torch.sin(y_position * div_term)\n",
    "        pe[3::4, :, :] = torch.cos(y_position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0), persistent=False)  # [1, C, H, W]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [N, C, H, W]\n",
    "        \"\"\"\n",
    "        return x + self.pe[:, :, :x.size(2), :x.size(3)]\n",
    "\n",
    "class PositionEncodingSine_line(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a sinusoidal position encoding that generalized to 1-dimensional sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, temp_bug_fix=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_shape (tuple): for 1/8 featmap, the max length of 256 corresponds to 2048 pixels\n",
    "            temp_bug_fix (bool): As noted in this [issue](https://github.com/zju3dv/LoFTR/issues/41),\n",
    "                the original implementation of LoFTR includes a bug in the pos-enc impl, which has little impact\n",
    "                on the final performance. For now, we keep both impls for backward compatability.\n",
    "                We will remove the buggy impl after re-training all variants of our released models.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if temp_bug_fix:\n",
    "            div_term = torch.exp(torch.arange(0, d_model//2, 2).float() * (-math.log(10000.0) / (d_model//2)))\n",
    "        else:  # a buggy implementation (for backward compatability only)\n",
    "            div_term = torch.exp(torch.arange(0, d_model//2, 2).float() * (-math.log(10000.0) / d_model//2))\n",
    "\n",
    "        self.div_term = div_term[:,None, None]  # [C//4, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, pts_int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [bs, C, L]\n",
    "            pts_int:[bs,L,2]\n",
    "        \"\"\"\n",
    "        d_model = x.shape[1]\n",
    "        x_position = pts_int[:,:,0].unsqueeze(0)\n",
    "        y_position = pts_int[:,:,0].unsqueeze(0)\n",
    "\n",
    "        pe = torch.zeros((x.shape[0],d_model, x.shape[2]))\n",
    "        print(pe.size())\n",
    "        b = torch.sin(x_position * self.div_term).permute((1,0,2))\n",
    "        print(b.size())\n",
    "        pe[:,0::4, :] = torch.sin(x_position * self.div_term).permute((1,0,2))\n",
    "        pe[:,1::4, :] = torch.cos(x_position * self.div_term).permute((1,0,2))\n",
    "        pe[:,2::4, :] = torch.sin(y_position * self.div_term).permute((1,0,2))\n",
    "        pe[:,3::4, :] = torch.cos(y_position * self.div_term).permute((1,0,2))\n",
    "        return x + pe\n",
    "\n",
    "\n",
    "model = PositionEncodingSine(16)\n",
    "x = torch.zeros(1,16,2,2)\n",
    "y = model(x)\n",
    "print('y size',y.size())\n",
    "# print(y)\n",
    "model2 = PositionEncodingSine_line(16)\n",
    "x = torch.zeros(2,16,4)\n",
    "\n",
    "pts_int = torch.tensor([[[  0.,   0.],\n",
    "         [  0.,   1.],\n",
    "         [  1.,   0.],[  1.,   1.]],\n",
    "               [[  1.,   1.],\n",
    "         [  1.,   0.],\n",
    "         [  1.,   1.],[  1.,   2.]]\n",
    "                        ])\n",
    "pts_int = pts_int +1\n",
    "\n",
    "y = model2(x,pts_int)\n",
    "print('y size',y.size())\n",
    "print(y)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[3., 3.]])"
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(1,2,3)\n",
    "print(a)\n",
    "a.sum(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4814]])\n",
      "tensor([[-0.3652]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[-0.1758]])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.rand(1,4,2)\n",
    "# print(a)\n",
    "bias = torch.randn(1,2)\n",
    "print(bias[:,0,None])\n",
    "# print(bias[:,0])\n",
    "# print( a[:,:,0].shape)\n",
    "# print( bias[:,0].shape)\n",
    "scale_x = torch.randn(1)\n",
    "print(scale_x[:,None])\n",
    "bias[:,0,None]*scale_x[:,None]\n",
    "# x = a[:,:,0] +bias[:,0,None]/scale_x[:,None]\n",
    "# y = a[:,:,1] ++bias[:,1,None]/scale_x[:,None]\n",
    "# z = torch.stack((x,y),dim=-1)\n",
    "# print(z)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}